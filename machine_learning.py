# -*- coding: utf-8 -*-
"""MACHINE LEARNING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Epw7ZzYjYyuV_NZJqlMbA0jyvIdw-vo2
"""

import numpy as np
import pandas as pd
import os
import glob
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras import layers, models

import os

base_path = "../Data-EEG-25-users-Neuromarketing"
data_path = os.path.join(base_path, "25-users")
label_path = os.path.join(base_path, "labels")

print("Data folder:", data_path)
print("Label folder:", label_path)

def load_lab_file(filepath):
    labels = []
    with open(filepath, "r") as f:
        for line in f:
            clean = line.strip()

            if clean == "":
                continue

            if clean.lower() == "like":
                labels.append(1)
            elif clean.lower() == "dislike":
                labels.append(0)
            elif clean.lower() == "neutral":
                labels.append(2)
            else:
                print("WARNING: label tidak dikenal:", clean)
                continue

    return np.array(labels)

example_lab = load_lab_file(os.path.join(label_path, "Abhishek_1.lab"))
print(example_lab)

def load_signal_txt(filepath):
    data = np.loadtxt(filepath)
    return data

example_signal = load_signal_txt(os.path.join(signal_path, "Abhishek_1.txt"))
print(example_signal.shape)
example_signal[:5]

X = []
y = []

signal_files = sorted(glob.glob(os.path.join(signal_path, "*.txt")))

for sfile in signal_files:
    fname = os.path.basename(sfile).replace(".txt", "")

    lfile = os.path.join(label_path, fname + ".lab")

    if not os.path.exists(lfile):
        print("Label tidak ditemukan untuk:", fname)
        continue


    signal = load_signal_txt(sfile)
    labels = load_lab_file(lfile)


    if signal.shape[0] != len(labels):
        print("WARNING mismatch:", fname, signal.shape[0], "vs", len(labels))
        min_len = min(signal.shape[0], len(labels))
        signal = signal[:min_len]
        labels = labels[:min_len]

    if len(signal) == 0 or len(labels) == 0:
        print("Skip data kosong:", fname)
        continue

    X.append(signal)
    y.append(labels)
if len(X) == 0 or len(y) == 0:
    print("ERROR: Tidak ada data valid untuk digabungkan!")
else:
    X = np.vstack(X)
    y = np.hstack(y)

    print("Total X shape =", X.shape)
    print("Total y shape =", y.shape)

scaler = StandardScaler()
X = scaler.fit_transform(X)
X = X.reshape(X.shape[0], X.shape[1], 1)

print("Final shape:", X.shape)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42)

print(X_train.shape, X_val.shape, X_test.shape)

model = models.Sequential([
    layers.Conv1D(32, 5, activation='relu', padding="same", input_shape=(X.shape[1], 1)),
    layers.BatchNormalization(),
    layers.MaxPooling1D(2),

    layers.Conv1D(64, 3, activation='relu', padding="same"),
    layers.BatchNormalization(),
    layers.MaxPooling1D(2),

    layers.LSTM(64),

    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),

    layers.Dense(1, activation='sigmoid')
])

model.compile(
    loss="binary_crossentropy",
    optimizer=tf.keras.optimizers.Adam(1e-3),
    metrics=["accuracy"]
)

model.summary()

history = model.fit(
    X_train, y_train,
    epochs=30,
    batch_size=32,
    validation_data=(X_val, y_val)
)

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.show()

plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.legend()
plt.show()

model.save("model.h5")

import pickle
with open("scaler.pkl", "wb") as f:
    pickle.dump(scaler, f)